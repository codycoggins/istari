# LLM routing configuration — maps task types to model tiers.
# Never hardcode model names in agent code; reference this config.

defaults:
  # Fallback model when no task-specific config matches
  model: ollama/llama3
  temperature: 0.7

tasks:
  chat_response:
    model: ollama/llama3
    temperature: 0.7
    description: User-facing chat responses — local for dev (swap to anthropic/claude for prod)

  summarization:
    model: ollama/llama3
    temperature: 0.3
    description: Email/content summarization — local, fast

  classification:
    model: ollama/llama3
    temperature: 0.0
    description: Content sensitivity classification — always local

  embedding:
    model: ollama/nomic-embed-text
    description: Vector embeddings — always local

  todo_enrichment:
    model: ollama/llama3
    temperature: 0.3
    description: TODO metadata inference — local for dev (swap to gemini for prod)

  prioritization:
    model: ollama/llama3
    temperature: 0.3
    description: Task prioritization — local for dev (swap to anthropic/claude for prod)

  digest_summary:
    model: ollama/llama3
    temperature: 0.3
    description: Digest summarization for proactive agent — emails + stale TODOs

  gmail_summary:
    model: ollama/llama3
    temperature: 0.3
    description: On-demand Gmail scan summary for chat responses
